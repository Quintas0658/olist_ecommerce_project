"""
Olistå–å®¶æ•°æ®å¤„ç†ç®¡é“

å®ç°ç«¯åˆ°ç«¯çš„æ•°æ®å¤„ç†æµç¨‹ï¼Œä»åŸå§‹æ•°æ®åˆ°åˆ†æå°±ç»ªçš„ç‰¹å¾æ•°æ®é›†ã€‚
å±•ç¤ºæ•°æ®å·¥ç¨‹ã€ETLå’Œç‰¹å¾å·¥ç¨‹èƒ½åŠ›ã€‚
"""

import pandas as pd
import numpy as np
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class OlistDataPipeline:
    """Olistæ•°æ®å¤„ç†ç®¡é“ç±»"""
    
    def __init__(self, data_path: str = "archive/"):
        """
        åˆå§‹åŒ–æ•°æ®ç®¡é“
        
        Args:
            data_path: åŸå§‹æ•°æ®æ–‡ä»¶è·¯å¾„
        """
        self.data_path = Path(data_path)
        self.processed_data = {}
        self.data_quality_report = {}
        
        # æ•°æ®æ–‡ä»¶æ˜ å°„
        self.data_files = {
            'sellers': 'olist_sellers_dataset.csv',
            'orders': 'olist_orders_dataset.csv',
            'order_items': 'olist_order_items_dataset.csv',
            'reviews': 'olist_order_reviews_dataset.csv',
            'products': 'olist_products_dataset.csv',
            'customers': 'olist_customers_dataset.csv',
            'payments': 'olist_order_payments_dataset.csv',
            'category_translation': 'product_category_name_translation.csv'
        }
    
    def run_full_pipeline(self, save_output: bool = True) -> Dict[str, pd.DataFrame]:
        """
        è¿è¡Œå®Œæ•´çš„æ•°æ®å¤„ç†ç®¡é“
        
        Args:
            save_output: æ˜¯å¦ä¿å­˜å¤„ç†åçš„æ•°æ®
            
        Returns:
            å¤„ç†åçš„æ•°æ®å­—å…¸
        """
        logger.info("ğŸš€ å¼€å§‹è¿è¡ŒOlistæ•°æ®å¤„ç†ç®¡é“...")
        
        # æ­¥éª¤1: æ•°æ®åŠ è½½
        self._load_raw_data()
        
        # æ­¥éª¤2: æ•°æ®è´¨é‡æ£€æŸ¥
        self._perform_data_quality_check()
        
        # æ­¥éª¤3: æ•°æ®æ¸…æ´—
        self._clean_data()
        
        # æ­¥éª¤4: ç‰¹å¾å·¥ç¨‹
        seller_features = self._build_seller_features()
        
        # æ­¥éª¤5: æ•°æ®éªŒè¯
        self._validate_processed_data(seller_features)
        
        # æ­¥éª¤6: ä¿å­˜å¤„ç†åçš„æ•°æ®
        if save_output:
            self._save_processed_data(seller_features)
        
        logger.info("âœ… æ•°æ®å¤„ç†ç®¡é“æ‰§è¡Œå®Œæˆ!")
        
        return {
            'seller_features': seller_features,
            'data_quality_report': self.data_quality_report
        }
    
    def _load_raw_data(self) -> None:
        """åŠ è½½åŸå§‹æ•°æ®æ–‡ä»¶"""
        logger.info("ğŸ“Š åŠ è½½åŸå§‹æ•°æ®æ–‡ä»¶...")
        
        for table_name, filename in self.data_files.items():
            file_path = self.data_path / filename
            try:
                df = pd.read_csv(file_path)
                self.processed_data[table_name] = df
                logger.info(f"  âœ… {table_name}: {df.shape[0]:,} è¡Œ x {df.shape[1]} åˆ—")
            except FileNotFoundError:
                logger.warning(f"  âš ï¸ æ–‡ä»¶æœªæ‰¾åˆ°: {filename}")
            except Exception as e:
                logger.error(f"  âŒ åŠ è½½ {filename} æ—¶å‡ºé”™: {str(e)}")
    
    def _perform_data_quality_check(self) -> None:
        """æ‰§è¡Œæ•°æ®è´¨é‡æ£€æŸ¥"""
        logger.info("ğŸ” æ‰§è¡Œæ•°æ®è´¨é‡æ£€æŸ¥...")
        
        quality_report = {}
        
        for table_name, df in self.processed_data.items():
            table_report = {
                'row_count': len(df),
                'column_count': len(df.columns),
                'missing_values': df.isnull().sum().to_dict(),
                'missing_percentage': (df.isnull().sum() / len(df) * 100).round(2).to_dict(),
                'duplicate_rows': df.duplicated().sum(),
                'data_types': df.dtypes.astype(str).to_dict()
            }
            
            # æ£€æŸ¥å…³é”®å­—æ®µ
            if table_name == 'sellers' and 'seller_id' in df.columns:
                table_report['unique_sellers'] = df['seller_id'].nunique()
                table_report['duplicate_seller_ids'] = df['seller_id'].duplicated().sum()
            
            if table_name == 'orders' and 'order_id' in df.columns:
                table_report['unique_orders'] = df['order_id'].nunique()
                table_report['order_status_distribution'] = df['order_status'].value_counts().to_dict() if 'order_status' in df.columns else {}
            
            quality_report[table_name] = table_report
            
            # è®°å½•å…³é”®è´¨é‡é—®é¢˜
            high_missing_cols = [col for col, pct in table_report['missing_percentage'].items() if pct > 30]
            if high_missing_cols:
                logger.warning(f"  âš ï¸ {table_name} ä¸­ç¼ºå¤±å€¼è¶…è¿‡30%çš„åˆ—: {high_missing_cols}")
        
        self.data_quality_report = quality_report
        logger.info("  âœ… æ•°æ®è´¨é‡æ£€æŸ¥å®Œæˆ")
    
    def _clean_data(self) -> None:
        """æ•°æ®æ¸…æ´—"""
        logger.info("ğŸ§¹ å¼€å§‹æ•°æ®æ¸…æ´—...")
        
        # æ¸…æ´—è®¢å•æ•°æ®
        if 'orders' in self.processed_data:
            orders = self.processed_data['orders'].copy()
            
            # è½¬æ¢æ—¥æœŸåˆ—
            date_columns = [col for col in orders.columns if 'date' in col.lower() or 'timestamp' in col.lower()]
            for col in date_columns:
                try:
                    orders[col] = pd.to_datetime(orders[col])
                except:
                    logger.warning(f"    âš ï¸ æ— æ³•è½¬æ¢æ—¥æœŸåˆ—: {col}")
            
            # ç­›é€‰æœ‰æ•ˆè®¢å•çŠ¶æ€
            valid_statuses = ['delivered', 'shipped', 'processing', 'invoiced', 'approved']
            if 'order_status' in orders.columns:
                before_count = len(orders)
                orders = orders[orders['order_status'].isin(valid_statuses)]
                after_count = len(orders)
                logger.info(f"    ğŸ“¦ ç­›é€‰æœ‰æ•ˆè®¢å•: {before_count:,} â†’ {after_count:,}")
            
            self.processed_data['orders'] = orders
        
        # æ¸…æ´—è®¢å•å•†å“æ•°æ®
        if 'order_items' in self.processed_data:
            order_items = self.processed_data['order_items'].copy()
            
            # ç§»é™¤ä»·æ ¼å¼‚å¸¸çš„è®°å½•
            if 'price' in order_items.columns:
                before_count = len(order_items)
                order_items = order_items[(order_items['price'] > 0) & (order_items['price'] < 10000)]
                after_count = len(order_items)
                logger.info(f"    ğŸ’° ç­›é€‰åˆç†ä»·æ ¼åŒºé—´: {before_count:,} â†’ {after_count:,}")
            
            # è½¬æ¢æ—¥æœŸåˆ—
            if 'shipping_limit_date' in order_items.columns:
                try:
                    order_items['shipping_limit_date'] = pd.to_datetime(order_items['shipping_limit_date'])
                except:
                    logger.warning("    âš ï¸ æ— æ³•è½¬æ¢ shipping_limit_date")
            
            self.processed_data['order_items'] = order_items
        
        # æ¸…æ´—è¯„ä»·æ•°æ®
        if 'reviews' in self.processed_data:
            reviews = self.processed_data['reviews'].copy()
            
            # ç­›é€‰æœ‰æ•ˆè¯„åˆ†
            if 'review_score' in reviews.columns:
                before_count = len(reviews)
                reviews = reviews[reviews['review_score'].between(1, 5)]
                after_count = len(reviews)
                logger.info(f"    â­ ç­›é€‰æœ‰æ•ˆè¯„åˆ†(1-5): {before_count:,} â†’ {after_count:,}")
            
            # è½¬æ¢æ—¥æœŸåˆ—
            date_cols = ['review_creation_date', 'review_answer_timestamp']
            for col in date_cols:
                if col in reviews.columns:
                    try:
                        reviews[col] = pd.to_datetime(reviews[col])
                    except:
                        logger.warning(f"    âš ï¸ æ— æ³•è½¬æ¢æ—¥æœŸåˆ—: {col}")
            
            self.processed_data['reviews'] = reviews
        
        logger.info("  âœ… æ•°æ®æ¸…æ´—å®Œæˆ")
    
    def _build_seller_features(self) -> pd.DataFrame:
        """æ„å»ºå–å®¶ç‰¹å¾æ•°æ®é›†"""
        logger.info("ğŸ”§ æ„å»ºå–å®¶ç‰¹å¾æ•°æ®é›†...")
        
        # è·å–åŸºç¡€æ•°æ®
        sellers = self.processed_data['sellers'].copy()
        orders = self.processed_data['orders'].copy()
        order_items = self.processed_data['order_items'].copy()
        reviews = self.processed_data['reviews'].copy()
        products = self.processed_data.get('products', pd.DataFrame())
        
        # åˆå¹¶è®¢å•å’Œè®¢å•å•†å“æ•°æ®
        order_detail = orders.merge(order_items, on='order_id', how='inner')
        logger.info(f"  ğŸ“Š è®¢å•è¯¦æƒ…æ•°æ®: {len(order_detail):,} æ¡è®°å½•")
        
        # 1. åŸºç¡€ä¸šåŠ¡æŒ‡æ ‡
        logger.info("  ğŸ¢ è®¡ç®—åŸºç¡€ä¸šåŠ¡æŒ‡æ ‡...")
        business_metrics = self._calculate_business_metrics(order_detail)
        
        # 2. å®¢æˆ·æ»¡æ„åº¦æŒ‡æ ‡
        logger.info("  ğŸ˜Š è®¡ç®—å®¢æˆ·æ»¡æ„åº¦æŒ‡æ ‡...")
        satisfaction_metrics = self._calculate_satisfaction_metrics(reviews, order_items)
        
        # 3. è¿è¥æ•ˆç‡æŒ‡æ ‡
        logger.info("  âš¡ è®¡ç®—è¿è¥æ•ˆç‡æŒ‡æ ‡...")
        efficiency_metrics = self._calculate_efficiency_metrics(order_detail, orders)
        
        # 4. å•†å“ç»„åˆæŒ‡æ ‡
        logger.info("  ğŸ“¦ è®¡ç®—å•†å“ç»„åˆæŒ‡æ ‡...")
        product_metrics = self._calculate_product_metrics(order_items, products)
        
        # 5. æ—¶é—´åºåˆ—æŒ‡æ ‡
        logger.info("  ğŸ“ˆ è®¡ç®—æ—¶é—´åºåˆ—æŒ‡æ ‡...")
        temporal_metrics = self._calculate_temporal_metrics(order_detail)
        
        # åˆå¹¶æ‰€æœ‰æŒ‡æ ‡
        seller_features = sellers.copy()
        
        # æŒ‰é¡ºåºåˆå¹¶æŒ‡æ ‡
        for metrics_df, name in [
            (business_metrics, "ä¸šåŠ¡æŒ‡æ ‡"),
            (satisfaction_metrics, "æ»¡æ„åº¦æŒ‡æ ‡"),
            (efficiency_metrics, "æ•ˆç‡æŒ‡æ ‡"),
            (product_metrics, "å•†å“æŒ‡æ ‡"),
            (temporal_metrics, "æ—¶é—´åºåˆ—æŒ‡æ ‡")
        ]:
            seller_features = seller_features.merge(
                metrics_df, left_on='seller_id', right_index=True, how='left'
            )
            logger.info(f"    âœ… åˆå¹¶{name}: {len(metrics_df)} ä¸ªå–å®¶")
        
        # å¡«å……ç¼ºå¤±å€¼
        numeric_columns = seller_features.select_dtypes(include=[np.number]).columns
        seller_features[numeric_columns] = seller_features[numeric_columns].fillna(0)
        
        # æ·»åŠ è¡ç”Ÿç‰¹å¾
        seller_features = self._add_derived_features(seller_features)
        
        logger.info(f"  âœ… å–å®¶ç‰¹å¾æ„å»ºå®Œæˆ: {seller_features.shape}")
        
        return seller_features
    
    def _calculate_business_metrics(self, order_detail: pd.DataFrame) -> pd.DataFrame:
        """è®¡ç®—åŸºç¡€ä¸šåŠ¡æŒ‡æ ‡"""
        metrics = order_detail.groupby('seller_id').agg({
            'order_id': 'nunique',                    # æ€»è®¢å•æ•°
            'price': ['sum', 'mean', 'count'],        # é”€å”®é¢ã€å®¢å•ä»·ã€å•†å“ä»¶æ•°
            'freight_value': 'sum',                   # æ€»è¿è´¹
            'product_id': 'nunique',                  # å•†å“SKUæ•°
            'order_item_id': 'sum'                    # æ€»é”€å”®æ•°é‡
        }).round(2)
        
        # é‡å‘½ååˆ—
        metrics.columns = [
            'total_orders', 'total_revenue', 'avg_order_value', 'total_items_sold',
            'total_freight', 'unique_products', 'total_quantity'
        ]
        
        # è®¡ç®—è¡ç”ŸæŒ‡æ ‡
        metrics['avg_items_per_order'] = metrics['total_items_sold'] / metrics['total_orders']
        metrics['avg_price_per_item'] = metrics['total_revenue'] / metrics['total_items_sold']
        metrics['freight_ratio'] = metrics['total_freight'] / metrics['total_revenue']
        metrics['revenue_per_sku'] = metrics['total_revenue'] / metrics['unique_products']
        
        return metrics
    
    def _calculate_satisfaction_metrics(self, reviews: pd.DataFrame, order_items: pd.DataFrame) -> pd.DataFrame:
        """è®¡ç®—å®¢æˆ·æ»¡æ„åº¦æŒ‡æ ‡"""
        # åˆå¹¶è¯„ä»·å’Œå–å®¶ä¿¡æ¯
        review_seller = reviews.merge(
            order_items[['order_id', 'seller_id']], 
            on='order_id', 
            how='inner'
        )
        
        # åŸºç¡€æ»¡æ„åº¦æŒ‡æ ‡
        satisfaction_metrics = review_seller.groupby('seller_id').agg({
            'review_score': ['mean', 'count', 'std'],
            'review_id': 'count'
        }).round(3)
        
        satisfaction_metrics.columns = [
            'avg_review_score', 'total_reviews', 'review_score_std', 'review_count_check'
        ]
        satisfaction_metrics = satisfaction_metrics.drop('review_count_check', axis=1)
        
        # è®¡ç®—æ»¡æ„åº¦åˆ†å¸ƒ
        review_distribution = review_seller.groupby('seller_id')['review_score'].apply(
            lambda x: pd.Series({
                'score_1_rate': (x == 1).mean(),
                'score_2_rate': (x == 2).mean(),
                'score_3_rate': (x == 3).mean(),
                'score_4_rate': (x == 4).mean(),
                'score_5_rate': (x == 5).mean(),
                'positive_rate': (x >= 4).mean(),
                'negative_rate': (x <= 2).mean()
            })
        )
        
        # åˆå¹¶æ»¡æ„åº¦æŒ‡æ ‡
        satisfaction_metrics = satisfaction_metrics.merge(
            review_distribution, left_index=True, right_index=True, how='left'
        )
        
        return satisfaction_metrics
    
    def _calculate_efficiency_metrics(self, order_detail: pd.DataFrame, orders: pd.DataFrame) -> pd.DataFrame:
        """è®¡ç®—è¿è¥æ•ˆç‡æŒ‡æ ‡"""
        # è®¢å•å±¥çº¦ç›¸å…³æŒ‡æ ‡
        order_status_metrics = orders.groupby('customer_id').agg({
            'order_status': lambda x: (x == 'delivered').mean()
        }).rename(columns={'order_status': 'delivery_rate'})
        
        # æŒ‰å–å®¶èšåˆè¿è¥æ•ˆç‡
        efficiency_metrics = order_detail.groupby('seller_id').agg({
            'freight_value': 'mean',           # å¹³å‡è¿è´¹
            'price': 'std',                   # ä»·æ ¼ç¨³å®šæ€§
        }).round(3)
        
        efficiency_metrics.columns = ['avg_freight_per_order', 'price_volatility']
        
        # è®¡ç®—ä»·æ ¼ç¨³å®šæ€§æŒ‡æ ‡ï¼ˆå˜å¼‚ç³»æ•°ï¼‰
        price_stats = order_detail.groupby('seller_id')['price'].agg(['mean', 'std'])
        efficiency_metrics['price_cv'] = (price_stats['std'] / price_stats['mean']).fillna(0)
        
        return efficiency_metrics
    
    def _calculate_product_metrics(self, order_items: pd.DataFrame, products: pd.DataFrame) -> pd.DataFrame:
        """è®¡ç®—å•†å“ç»„åˆæŒ‡æ ‡"""
        # å•†å“å¤šæ ·æ€§æŒ‡æ ‡
        product_diversity = order_items.groupby('seller_id').agg({
            'product_id': ['nunique', 'count'],
            'price': ['min', 'max']
        })
        
        product_diversity.columns = [
            'unique_products_sold', 'total_products_sold', 
            'min_price', 'max_price'
        ]
        
        # ä»·æ ¼èŒƒå›´
        product_diversity['price_range'] = product_diversity['max_price'] - product_diversity['min_price']
        product_diversity['price_range_ratio'] = product_diversity['max_price'] / product_diversity['min_price']
        
        # å•†å“å‘¨è½¬ç‡
        product_diversity['product_turnover'] = product_diversity['total_products_sold'] / product_diversity['unique_products_sold']
        
        return product_diversity
    
    def _calculate_temporal_metrics(self, order_detail: pd.DataFrame) -> pd.DataFrame:
        """è®¡ç®—æ—¶é—´åºåˆ—æŒ‡æ ‡"""
        # ç¡®ä¿æœ‰æ—¶é—´åˆ—
        if 'order_purchase_timestamp' not in order_detail.columns:
            # å¦‚æœæ²¡æœ‰æ—¶é—´æˆ³ï¼Œè¿”å›ç©ºçš„DataFrame
            empty_metrics = pd.DataFrame(index=order_detail['seller_id'].unique())
            empty_metrics['days_active'] = 365  # é»˜è®¤å€¼
            empty_metrics['avg_orders_per_month'] = 0
            return empty_metrics
        
        # è½¬æ¢æ—¶é—´æˆ³
        order_detail['order_date'] = pd.to_datetime(order_detail['order_purchase_timestamp']).dt.date
        
        # è®¡ç®—æ´»è·ƒå¤©æ•°å’Œè®¢å•é¢‘ç‡
        temporal_metrics = order_detail.groupby('seller_id').agg({
            'order_date': ['min', 'max', 'nunique'],
            'order_id': 'nunique'
        })
        
        temporal_metrics.columns = ['first_order_date', 'last_order_date', 'active_days', 'unique_orders']
        
        # è®¡ç®—æ´»è·ƒæœŸé—´é•¿åº¦ï¼ˆå¤©ï¼‰
        temporal_metrics['days_span'] = (
            pd.to_datetime(temporal_metrics['last_order_date']) - 
            pd.to_datetime(temporal_metrics['first_order_date'])
        ).dt.days + 1
        
        # è®¢å•é¢‘ç‡æŒ‡æ ‡
        temporal_metrics['avg_orders_per_month'] = temporal_metrics['unique_orders'] / (temporal_metrics['days_span'] / 30)
        temporal_metrics['avg_gap_between_orders'] = temporal_metrics['days_span'] / temporal_metrics['unique_orders']
        
        return temporal_metrics[['days_span', 'active_days', 'avg_orders_per_month', 'avg_gap_between_orders']]
    
    def _add_derived_features(self, seller_features: pd.DataFrame) -> pd.DataFrame:
        """æ·»åŠ è¡ç”Ÿç‰¹å¾"""
        logger.info("  ğŸ¯ æ·»åŠ è¡ç”Ÿç‰¹å¾...")
        
        # 1. ä¸šåŠ¡æˆç†Ÿåº¦è¯„åˆ†
        seller_features['business_maturity_score'] = (
            np.log1p(seller_features['total_orders']) * 0.3 +
            np.log1p(seller_features['unique_products']) * 0.2 +
            np.minimum(seller_features.get('days_span', 365) / 365, 1) * 0.3 +
            np.minimum(seller_features['total_reviews'] / 50, 1) * 0.2
        )
        
        # 2. é£é™©è¯„åˆ†ï¼ˆè¶Šä½è¶Šå¥½ï¼‰
        seller_features['risk_score'] = (
            (5 - seller_features['avg_review_score'].fillna(3)) / 4 * 0.4 +
            seller_features['negative_rate'].fillna(0.2) * 0.3 +
            np.minimum(seller_features['freight_ratio'].fillna(0.15) / 0.3, 1) * 0.2 +
            np.minimum(seller_features.get('price_cv', 0.5), 1) * 0.1
        )
        
        # 3. æˆé•¿æ½œåŠ›è¯„åˆ†
        revenue_rank = seller_features['total_revenue'].rank(pct=True)
        order_rank = seller_features['total_orders'].rank(pct=True)
        
        seller_features['growth_potential_score'] = (
            (1 - revenue_rank) * 0.3 +  # ä½æ”¶å…¥=é«˜æ½œåŠ›
            np.minimum(seller_features['unique_products'] / 20, 1) * 0.2 +
            seller_features['avg_review_score'].fillna(3) / 5 * 0.3 +
            np.minimum(seller_features.get('avg_orders_per_month', 1), 5) / 5 * 0.2
        )
        
        # 4. åˆ†ç±»æ ‡ç­¾
        seller_features['seller_size'] = pd.cut(
            seller_features['total_revenue'],
            bins=[0, 1000, 5000, 20000, np.inf],
            labels=['Micro', 'Small', 'Medium', 'Large']
        )
        
        seller_features['activity_level'] = pd.cut(
            seller_features['total_orders'],
            bins=[0, 5, 20, 100, np.inf],
            labels=['Inactive', 'Low', 'Medium', 'High']
        )
        
        return seller_features
    
    def _validate_processed_data(self, seller_features: pd.DataFrame) -> None:
        """éªŒè¯å¤„ç†åçš„æ•°æ®"""
        logger.info("âœ… éªŒè¯å¤„ç†åçš„æ•°æ®...")
        
        # åŸºç¡€éªŒè¯
        assert len(seller_features) > 0, "å–å®¶ç‰¹å¾æ•°æ®ä¸ºç©º"
        assert 'seller_id' in seller_features.columns, "ç¼ºå°‘seller_idåˆ—"
        
        # æ•°æ®åˆç†æ€§æ£€æŸ¥
        numeric_cols = seller_features.select_dtypes(include=[np.number]).columns
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸å€¼
        for col in ['total_revenue', 'total_orders', 'avg_review_score']:
            if col in seller_features.columns:
                q99 = seller_features[col].quantile(0.99)
                outliers = (seller_features[col] > q99 * 10).sum()
                if outliers > 0:
                    logger.warning(f"  âš ï¸ {col} å­˜åœ¨ {outliers} ä¸ªæç«¯å¼‚å¸¸å€¼")
        
        # æ£€æŸ¥è¯„åˆ†èŒƒå›´
        if 'avg_review_score' in seller_features.columns:
            invalid_scores = seller_features[
                (seller_features['avg_review_score'] < 1) | 
                (seller_features['avg_review_score'] > 5)
            ]
            if len(invalid_scores) > 0:
                logger.warning(f"  âš ï¸ å‘ç° {len(invalid_scores)} ä¸ªæ— æ•ˆè¯„åˆ†")
        
        logger.info(f"  âœ… æ•°æ®éªŒè¯å®Œæˆ: {seller_features.shape}")
    
    def _save_processed_data(self, seller_features: pd.DataFrame) -> None:
        """ä¿å­˜å¤„ç†åçš„æ•°æ®"""
        logger.info("ğŸ’¾ ä¿å­˜å¤„ç†åçš„æ•°æ®...")
        
        # åˆ›å»ºdataç›®å½•
        output_dir = Path('data')
        output_dir.mkdir(exist_ok=True)
        
        # ä¿å­˜å–å®¶ç‰¹å¾æ•°æ®
        seller_features.to_csv(output_dir / 'seller_features_processed.csv', index=False)
        logger.info(f"  âœ… å–å®¶ç‰¹å¾æ•°æ®ä¿å­˜è‡³: {output_dir / 'seller_features_processed.csv'}")
        
        # ä¿å­˜æ•°æ®è´¨é‡æŠ¥å‘Š
        quality_report_df = pd.DataFrame(self.data_quality_report).T
        quality_report_df.to_csv(output_dir / 'data_quality_report.csv')
        logger.info(f"  âœ… æ•°æ®è´¨é‡æŠ¥å‘Šä¿å­˜è‡³: {output_dir / 'data_quality_report.csv'}")
        
        # ä¿å­˜æ•°æ®å­—å…¸
        data_dictionary = self._generate_data_dictionary(seller_features)
        data_dictionary.to_csv(output_dir / 'data_dictionary.csv', index=False)
        logger.info(f"  âœ… æ•°æ®å­—å…¸ä¿å­˜è‡³: {output_dir / 'data_dictionary.csv'}")
    
    def _generate_data_dictionary(self, seller_features: pd.DataFrame) -> pd.DataFrame:
        """ç”Ÿæˆæ•°æ®å­—å…¸"""
        data_dict = []
        
        # å®šä¹‰å­—æ®µæè¿°
        field_descriptions = {
            'seller_id': 'å–å®¶å”¯ä¸€æ ‡è¯†ç¬¦',
            'seller_zip_code_prefix': 'å–å®¶é‚®ç¼–å‰ç¼€',
            'seller_city': 'å–å®¶åŸå¸‚',
            'seller_state': 'å–å®¶å·',
            'total_orders': 'æ€»è®¢å•æ•°',
            'total_revenue': 'æ€»é”€å”®é¢(R$)',
            'avg_order_value': 'å¹³å‡è®¢å•ä»·å€¼(R$)',
            'total_items_sold': 'æ€»é”€å”®å•†å“ä»¶æ•°',
            'total_freight': 'æ€»è¿è´¹(R$)',
            'unique_products': 'å•†å“SKUæ•°é‡',
            'total_quantity': 'æ€»é”€å”®æ•°é‡',
            'avg_items_per_order': 'å¹³å‡æ¯å•å•†å“ä»¶æ•°',
            'avg_price_per_item': 'å¹³å‡å•†å“å•ä»·(R$)',
            'freight_ratio': 'è¿è´¹å é”€å”®é¢æ¯”ä¾‹',
            'revenue_per_sku': 'æ¯SKUå¹³å‡æ”¶å…¥(R$)',
            'avg_review_score': 'å¹³å‡å®¢æˆ·è¯„åˆ†(1-5)',
            'total_reviews': 'æ€»è¯„ä»·æ•°é‡',
            'review_score_std': 'è¯„åˆ†æ ‡å‡†å·®',
            'positive_rate': 'å¥½è¯„ç‡(4-5åˆ†)',
            'negative_rate': 'å·®è¯„ç‡(1-2åˆ†)',
            'business_maturity_score': 'ä¸šåŠ¡æˆç†Ÿåº¦è¯„åˆ†(0-1)',
            'risk_score': 'é£é™©è¯„åˆ†(0-1ï¼Œè¶Šä½è¶Šå¥½)',
            'growth_potential_score': 'æˆé•¿æ½œåŠ›è¯„åˆ†(0-1)',
            'seller_size': 'å–å®¶è§„æ¨¡åˆ†ç±»',
            'activity_level': 'æ´»è·ƒåº¦ç­‰çº§'
        }
        
        for col in seller_features.columns:
            dtype = str(seller_features[col].dtype)
            description = field_descriptions.get(col, 'å¾…è¡¥å……æè¿°')
            
            data_dict.append({
                'Field': col,
                'Type': dtype,
                'Description': description,
                'Non_Null_Count': seller_features[col].count(),
                'Null_Count': seller_features[col].isnull().sum(),
                'Unique_Values': seller_features[col].nunique() if dtype != 'object' else 'N/A'
            })
        
        return pd.DataFrame(data_dict)


def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºæ•°æ®ç®¡é“ä½¿ç”¨"""
    # åˆå§‹åŒ–æ•°æ®ç®¡é“
    pipeline = OlistDataPipeline()
    
    # è¿è¡Œå®Œæ•´ç®¡é“
    results = pipeline.run_full_pipeline(save_output=True)
    
    # æ˜¾ç¤ºç»“æœæ‘˜è¦
    seller_features = results['seller_features']
    print(f"\nğŸ¯ æ•°æ®ç®¡é“æ‰§è¡Œç»“æœ:")
    print(f"  ğŸ“Š å¤„ç†åçš„å–å®¶æ•°é‡: {len(seller_features):,}")
    print(f"  ğŸ“‹ ç‰¹å¾æ•°é‡: {len(seller_features.columns)}")
    print(f"  ğŸ¯ æ´»è·ƒå–å®¶: {(seller_features['total_orders'] > 0).sum():,}")
    print(f"  ğŸ’° å¹³å°æ€»GMV: R$ {seller_features['total_revenue'].sum():,.2f}")
    
    return seller_features

if __name__ == "__main__":
    main() 